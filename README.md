I have implemented CycleGAN from scratch to translate my images into Monet paintings.

# CycleGAN
CycleGAN translates images from one domain into another
Read this paper: https://junyanz.github.io/CycleGAN/

## How CycleGAN is trained in one epoch:? 

# In total we have two generators and two discriminators for two domains.
--Define models for generator and discriminator 

# Generators:
G: Translates images from Domain X (e.g., photos) to Domain Y (e.g., Monet paintings).<br />
F: Translates images from Domain Y back to Domain X.<br />

# Discriminators:
D_X: Distinguishes between real images in Domain X and those generated by F.<br />
D_Y: Distinguishes between real images in Domain Y and those generated by G.<br />

Hereâ€™s a step-by-step breakdown of what occurs during each epoch and how the gradient is calculated:<br />
--We are computing gradients for G & F, D_M (D_X), D_P (D_Y)<br />
--gradients_G captures gradients for both G and F since they are concatenated<br />
--optimizer_G updates both G and F simultaneously to ensure synchronized updates.optimizer_D_M and optimizer_D_P update their respective discriminators.<br />

with tf.GradientTape(persistent=True) as tape:
    # Generate fake images
    fake_monet = G(photo, training=True)
    cycled_photo = F(fake_monet, training=True)
    
    fake_photo = F(monet, training=True)
    cycled_monet = G(fake_photo, training=True)
    
    # Identity mapping (optional)
    same_monet = G(monet, training=True)
    same_photo = F(photo, training=True)
    
    # Discriminator outputs
    D_X_real = D_X(photo, training=True)
    D_X_fake = D_X(fake_photo, training=True)
    
    D_Y_real = D_Y(monet, training=True)
    D_Y_fake = D_Y(fake_monet, training=True)
    
    # Compute adversarial losses
    adv_loss_G = bce_loss(tf.ones_like(D_Y_fake), D_Y_fake)
    adv_loss_F = bce_loss(tf.ones_like(D_X_fake), D_X_fake)
    
    # Compute cycle consistency losses
    cycle_loss_G = cycle_loss_fn(photo, cycled_photo)
    cycle_loss_F = cycle_loss_fn(monet, cycled_monet)
    
    # Compute identity losses
    identity_loss_G = cycle_loss_fn(monet, same_monet)
    identity_loss_F = cycle_loss_fn(photo, same_photo)
    
    # Total generator losses
    total_gen_loss_G = adv_loss_G + 10 * cycle_loss_G + 5 * identity_loss_G
    total_gen_loss_F = adv_loss_F + 10 * cycle_loss_F + 5 * identity_loss_F
    
    # Total generator loss
    total_gen_loss = total_gen_loss_G + total_gen_loss_F
    
    # Compute discriminator losses
    D_X_loss_real = bce_loss(tf.ones_like(D_X_real), D_X_real)
    D_X_loss_fake = bce_loss(tf.zeros_like(D_X_fake), D_X_fake)
    D_X_loss = (D_X_loss_real + D_X_loss_fake) * 0.5
    
    D_Y_loss_real = bce_loss(tf.ones_like(D_Y_real), D_Y_real)
    D_Y_loss_fake = bce_loss(tf.zeros_like(D_Y_fake), D_Y_fake)
    D_Y_loss = (D_Y_loss_real + D_Y_loss_fake) * 0.5

# Compute gradients
gradients_G = tape.gradient(total_gen_loss, G.trainable_variables + F.trainable_variables)
gradients_D_X = tape.gradient(D_X_loss, D_X.trainable_variables)
gradients_D_Y = tape.gradient(D_Y_loss, D_Y.trainable_variables)

# Apply gradients
optimizer_G.apply_gradients(zip(gradients_G, G.trainable_variables + F.trainable_variables))
optimizer_D_X.apply_gradients(zip(gradients_D_X, D_X.trainable_variables))
optimizer_D_Y.apply_gradients(zip(gradients_D_Y, D_Y.trainable_variables))


return total_gen_loss, D_X_loss, D_Y_loss

I have implemented CycleGAN from scratch to translate my images into Monet paintings.

# CycleGAN
CycleGAN translates images from one domain into another
Read this paper: https://junyanz.github.io/CycleGAN/

## How CycleGAN is trained in one epoch:? 

# In total we have two generators and two discriminators for two domains.
--Define models for generator and discriminator 

# Generators:
G: Translates images from Domain X (e.g., photos) to Domain Y (e.g., Monet paintings).<br />
F: Translates images from Domain Y back to Domain X.\

# Discriminators:
D_X: Distinguishes between real images in Domain X and those generated by F.\\
D_Y: Distinguishes between real images in Domain Y and those generated by G.\\

Hereâ€™s a step-by-step breakdown of what occurs during each epoch and how the gradient is calculated:\\
--We are computing gradients for G & F, D_M (D_X), D_P (D_Y)\\
--gradients_G captures gradients for both G and F since they are concatenated\\
--optimizer_G updates both G and F simultaneously to ensure synchronized updates.optimizer_D_M and optimizer_D_P update their respective discriminators.\\

with tf.GradientTape(persistent=True) as tape:
        # Generate fake Monet images
        fake_monet = G(photo, training=True)
        # Generate cycled photos
        cycled_photo = F(fake_monet, training=True)
        
        # Generate fake photos
        fake_photo = F(monet, training=True)
        # Generate cycled Monet images
        cycled_monet = G(fake_photo, training=True)
        
        # Identity mapping (optional)
        same_monet = G(monet, training=True)
        same_photo = F(photo, training=True)
        
        # Discriminator outputs
        D_M_real = D_M(monet, training=True)
        D_M_fake = D_M(fake_monet, training=True)
        
        D_P_real = D_P(photo, training=True)
        D_P_fake = D_P(fake_photo, training=True)
        
        # Adversarial losses
        adv_loss_G = bce_loss(tf.ones_like(D_M_fake), D_M_fake)
        adv_loss_F = bce_loss(tf.ones_like(D_P_fake), D_P_fake)
        
        # Cycle consistency losses
        cycle_loss_G = cycle_loss_fn(photo, cycled_photo)
        cycle_loss_F = cycle_loss_fn(monet, cycled_monet)
        
        # Identity losses
        identity_loss_G = cycle_loss_fn(monet, same_monet)
        identity_loss_F = cycle_loss_fn(photo, same_photo)
        
        # Total generator losses
        total_gen_loss_G = adv_loss_G + 10 * cycle_loss_G + 5 * identity_loss_G
        total_gen_loss_F = adv_loss_F + 10 * cycle_loss_F + 5 * identity_loss_F
        
        # Total generator loss
        total_gen_loss = total_gen_loss_G + total_gen_loss_F
        
        # Discriminator losses
        D_M_loss_real = bce_loss(tf.ones_like(D_M_real), D_M_real)
        D_M_loss_fake = bce_loss(tf.zeros_like(D_M_fake), D_M_fake)
        D_M_loss = (D_M_loss_real + D_M_loss_fake) * 0.5
        
        D_P_loss_real = bce_loss(tf.ones_like(D_P_real), D_P_real)
        D_P_loss_fake = bce_loss(tf.zeros_like(D_P_fake), D_P_fake)
        D_P_loss = (D_P_loss_real + D_P_loss_fake) * 0.5
        
    # Compute gradients
    gradients_G = tape.gradient(total_gen_loss, G.trainable_variables + F.trainable_variables)
    gradients_D_M = tape.gradient(D_M_loss, D_M.trainable_variables)
    gradients_D_P = tape.gradient(D_P_loss, D_P.trainable_variables)
    
    # Apply gradients
    optimizer_G.apply_gradients(zip(gradients_G, G.trainable_variables + F.trainable_variables))
    optimizer_D_M.apply_gradients(zip(gradients_D_M, D_M.trainable_variables))
    optimizer_D_P.apply_gradients(zip(gradients_D_P, D_P.trainable_variables))    
    
    return total_gen_loss, D_M_loss, D_P_loss
